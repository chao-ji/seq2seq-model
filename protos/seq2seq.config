prediction_model {
  unit_type: "lstm"
  num_units: 512
  forget_bias: 1.0
  keep_prob: 0.8
  encoder_type: "bi"
  time_major: true
  share_vocab: false

  initializer {
    uniform {
      min_val: -0.1
      max_val: 0.1
    }
  }

  num_encoder_layers: 2
  num_encoder_res_layers: 0
  num_decoder_layers: 2
  num_decoder_res_layers: 0
  attention_type: "scaled_luong"
  output_attention: true
}

dataset {
  batch_size: 128
  shuffle_buffer_size: 128000
  num_buckets: 5
  src_max_len: 50
  tgt_max_len: 50
  sos: "<s>"
  eos: "</s>"
}

decoding {
  beam_width: 10
  length_penalty_weight: 0.0
  sampling_temperature: 1.0
}

optimization {
  base_learning_rate: 1.0
  warmup_scheme: "t2t"
  warmup_steps: 0
  decay_scheme: "luong234"
  optimizer: "sgd"
  num_train_steps: 12516
  max_grad_norm: 5.0
}

src_vocab_size: 7709

tgt_vocab_size: 17191

random_seed: 0

steps_per_stats: 100

steps_ckpt: 1000

