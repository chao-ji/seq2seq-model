prediction_model {
  # "lstm", "gru" or "nas"
  unit_type: "lstm"   
  num_units: 512
  forget_bias: 1.0
  keep_prob: 0.8

  # "uni" for unidirectional RNN, "bi" for bidirectional RNN
  encoder_type: "bi"  
  time_major: true
  share_vocab: false

  initializer {
    uniform {
      min_val: -0.1
      max_val: 0.1
    }
  }

  num_encoder_layers: 2
  num_encoder_res_layers: 0
  num_decoder_layers: 2
  num_decoder_res_layers: 0

  # "luong", "scaled luong", "bahdanau", "normed_bahdanau". 
  # Leave is out for a vanilla model
  attention_type: "scaled_luong" 
  output_attention: true
}

dataset {
  batch_size: 128
  shuffle_buffer_size: 128000
  num_buckets: 5
  src_max_len: 50
  tgt_max_len: 50
  sos: "<s>"
  eos: "</s>"
}

decoding {

  # If > 0, beam search is enabled for decoding
  beam_width: 10                

  # Length penalty for beam search
  length_penalty_weight: 0.0

  # If > 0 and beam search is disabled, sampling is used for decoding. 
  # If <= 0, greedy is used for decoding.
  sampling_temperature: 1.0     
}

optimization {
  base_learning_rate: 1.0
  warmup_scheme: "t2t"
  warmup_steps: 0
  decay_scheme: "luong234"
  optimizer: "sgd"
  num_train_steps: 12516
  max_grad_norm: 5.0
}

src_vocab_size: 17191 # 7709

tgt_vocab_size: 7709 # 17191

random_seed: 0

# print out stats per `steps_per_states` steps
steps_per_stats: 100

# save to checkpoint files per `steps_ckpt` steps
steps_ckpt: 1000 

